# app.py

import os
import re
import tempfile
import zipfile
import io
import shutil
import pdfplumber
from docx import Document
from openpyxl import load_workbook
from collections import defaultdict
import streamlit as st

# --- Streamlit UI é…ç½® ---
st.set_page_config(page_title="å•†æ ‡ç”³è¯·è¯·æ¬¾å•ç”Ÿæˆå™¨", layout="wide")
st.title("ğŸ“ å•†æ ‡ç”³è¯·è¯·æ¬¾å•ä¸å‘ç¥¨ç”³è¯·è¡¨ç”Ÿæˆå™¨")

# --- è¾…åŠ©å‡½æ•° ---

def number_to_upper(amount):
    """é‡‘é¢è½¬å¤§å†™ï¼ˆæ”¯æŒä¸‡ã€åƒç­‰å•ä½ï¼‰"""
    CN_NUM = ['é›¶', 'å£¹', 'è´°', 'å', 'è‚†', 'ä¼', 'é™†', 'æŸ’', 'æŒ', 'ç–']
    CN_UNIT = ['å…ƒ', 'æ‹¾', 'ä½°', 'ä»Ÿ', 'ä¸‡', 'æ‹¾ä¸‡', 'ä½°ä¸‡', 'ä»Ÿä¸‡', 'äº¿']
    s = str(int(amount))[::-1]
    result = []
    for i in range(len(s)):
        digit = int(s[i])
        unit = CN_UNIT[i] if i < len(CN_UNIT) else ''
        if digit != 0:
            result.append(f"{CN_NUM[digit]}{unit}")
        else:
            if i == 0 and not result:
                result.append("é›¶")
    formatted = ''.join(reversed(result))
    return formatted + "å…ƒæ•´"

def create_word_doc(data, agent_fee, categories, template_path, output_path):
    """ç”ŸæˆWordè¯·æ¬¾å•"""
    try:
        doc = Document(template_path)
    except Exception as e:
        raise Exception(f"æ— æ³•åŠ è½½Wordæ¨¡æ¿ '{template_path}': {e}")

    num_items = len(categories)
    total_official = num_items * 270
    total_agent = num_items * agent_fee
    total_subtotal = total_official + total_agent
    total_upper = number_to_upper(total_subtotal)
    
    # æ›¿æ¢æ®µè½å ä½ç¬¦
    for para in doc.paragraphs:
        if "{ç”³è¯·äºº}" in para.text:
            para.text = para.text.replace("{ç”³è¯·äºº}", data["ç”³è¯·äºº"])
        if "{äº‹å®œç±»å‹}" in para.text:
            para.text = para.text.replace("{äº‹å®œç±»å‹}", "å•†æ ‡æ³¨å†Œç”³è¯·")
        if "{æ—¥æœŸ}" in para.text:
            para.text = para.text.replace("{æ—¥æœŸ}", data["æ—¥æœŸ"])
        if "åˆè®¡ï¼š" in para.text:
            para.text = para.text.replace("{æ€»å®˜è´¹}", str(total_official))
            para.text = para.text.replace("{æ€»ä»£ç†è´¹}", str(total_agent))
            para.text = para.text.replace("{æ€»è®¡}", str(total_subtotal))
            para.text = para.text.replace("{å¤§å†™}", total_upper)
            
    # å¤„ç†è¡¨æ ¼
    table = doc.tables[0]
    if len(table.rows) > 2:
        row_to_delete = table.rows[1]
        tbl = row_to_delete._element
        tbl.getparent().remove(tbl)

    # æ·»åŠ å•†æ ‡ä¿¡æ¯è¡Œ
    for idx, item in enumerate(categories, 1):
        row = table.add_row().cells
        row[0].text = str(idx)  # åºå·
        row[1].text = item['å•†æ ‡åç§°']  # å•†æ ‡åç§°
        row[2].text = "å•†æ ‡æ³¨å†Œç”³è¯·"  # äº‹å®œ
        row[3].text = item['ç±»åˆ«']  # ç±»åˆ«
        row[4].text = f"Â¥{270}"  # å®˜è´¹
        row[5].text = f"Â¥{agent_fee}"  # ä»£ç†è´¹
        row[6].text = f"Â¥{270 + agent_fee}"  # å°è®¡

    # æ·»åŠ åˆè®¡è¡Œ
    total_row = table.add_row().cells
    total_row[0].merge(total_row[3])  # åˆå¹¶å‰å››ä¸ªå•å…ƒæ ¼ (åºå·ã€äº‹å®œã€å•†æ ‡åç§°ã€ç±»åˆ«)
    total_row[0].text = "åˆè®¡"
    total_row[0].paragraphs[0].alignment = 1  # å±…ä¸­å¯¹é½
    total_row[4].text = f"Â¥{total_official}"  # æ€»å®˜è´¹
    total_row[5].text = f"Â¥{total_agent}"  # æ€»ä»£ç†è´¹
    total_row[6].text = f"Â¥{total_subtotal}"  # æ€»è®¡

    # æ›´æ–°æ–‡ä»¶åä»¥åŒ…å«ç”³è¯·äººå’Œæ—¥æœŸ
    filename = f"è¯·æ¬¾å•ï¼ˆ{data['ç”³è¯·äºº']}-å•†æ ‡æ³¨å†Œç”³è¯·-{total_subtotal}-{data['æ—¥æœŸ']}ï¼‰.docx"
    full_output_path = os.path.join(output_path, filename)
    try:
        doc.save(full_output_path)
        return full_output_path
    except Exception as e:
        raise Exception(f"ä¿å­˜Wordæ–‡ä»¶ '{full_output_path}' å¤±è´¥: {e}")

# --- æ ¸å¿ƒå¤„ç†å‡½æ•° ---

def extract_pdf_data_streamlit(uploaded_file):
    """ä»Streamlit UploadedFileå¯¹è±¡æå–æ•°æ®ï¼Œæ¨¡æ‹Ÿé¡ºåºé˜…è¯»ã€‚"""
    applicant = "N/A"
    unified_credit_code = "N/A"
    final_date = "N/A"
    trademarks_with_categories = []
    pending_categories = []

    # ä½¿ç”¨ tempfile å¤„ç† UploadedFile å¯¹è±¡
    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
        tmp_file.write(uploaded_file.getvalue())
        tmp_file_path = tmp_file.name

    try:
        with pdfplumber.open(tmp_file_path) as pdf:
            all_texts = [page.extract_text().replace("ã€€", " ").replace("\xa0", " ").strip() for page in pdf.pages]
            all_text_combined = "\n---PAGE_BREAK---\n".join(all_texts)
        pages = all_text_combined.split("\n---PAGE_BREAK---\n")

        for page_num, page_text in enumerate(pages):
            if page_num == 0:
                # --- ç¬¬ä¸€é¡µï¼šæå–ç”³è¯·äººå’Œç»Ÿä¸€ç¤¾ä¼šä¿¡ç”¨ä»£ç  ---
                applicant_match = re.search(r"ç”³è¯·äººåç§°\(ä¸­æ–‡\)ï¼š\s*(.*?)\s*\(\s*è‹±æ–‡\)", page_text)
                applicant = applicant_match.group(1).strip() if applicant_match else "N/A"

                # --- ä¿®æ”¹ç‚¹ï¼šè°ƒæ•´ç»Ÿä¸€ç¤¾ä¼šä¿¡ç”¨ä»£ç çš„æ­£åˆ™è¡¨è¾¾å¼ä»¥åŒ…å«å¯èƒ½çš„å­—æ¯ ---
                unified_credit_code_match = re.search(r"ç»Ÿä¸€ç¤¾ä¼šä¿¡ç”¨ä»£ç ï¼š\s*([0-9A-Z]+)", page_text)
                unified_credit_code = unified_credit_code_match.group(1).strip() if unified_credit_code_match else "N/A"
                # --- ä¿®æ”¹ç‚¹ç»“æŸ ---

                # å°è¯•ä»ç¬¬ä¸€é¡µæå–æ—¥æœŸä½œä¸ºåå¤‡
                if final_date == "N/A":
                    date_match = re.search(r"(\d{4}å¹´\s*\d{1,2}æœˆ\s*\d{1,2}æ—¥)", page_text)
                    final_date = date_match.group(1).replace(" ", "") if date_match else "N/A"
                continue # å¤„ç†å®Œç¬¬ä¸€é¡µï¼Œç»§ç»­ä¸‹ä¸€é¡µ

            # --- åç»­é¡µé¢ï¼šæå–ç±»åˆ«æˆ–å•†æ ‡å ---
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«ç±»åˆ«ä¿¡æ¯
            if re.search(r'ç±»åˆ«ï¼š\d+', page_text):
                categories_found = re.findall(r'ç±»åˆ«ï¼š(\d+)', page_text)
                pending_categories.extend(categories_found)
                # print(f"  -> åœ¨ç¬¬ {page_num + 1} é¡µæ‰¾åˆ°ç±»åˆ«: {categories_found}") # è°ƒè¯•ä¿¡æ¯

            # æ£€æŸ¥æ˜¯å¦åŒ…å«å§”æ‰˜ä¹¦
            elif 'å•†æ ‡ä»£ç†å§”æ‰˜ä¹¦' in page_text:
                # --- ä¿®æ”¹ç‚¹ï¼šè°ƒæ•´æå–å•†æ ‡åç§°çš„æ­£åˆ™è¡¨è¾¾å¼ ---
                # åŸæ­£åˆ™: r'å•†æ ‡ä»£ç†å§”æ‰˜ä¹¦.*?ä»£ç†\s+(.*?)\s+å•†æ ‡çš„å¦‚ä¸‹.*?äº‹å®œ'
                # é—®é¢˜ï¼š\s+ è¦æ±‚æ•è·å†…å®¹åç´§è·Ÿç©ºæ ¼ï¼Œä½†å®é™…æ˜¯ç´§è·Ÿ 'å•†æ ‡' å­—ã€‚
                # æ–°æ­£åˆ™: æ›´å‡†ç¡®åœ°å®šä½åˆ° 'å•†æ ‡' å­—
                tm_name_match = re.search(r'å•†æ ‡ä»£ç†å§”æ‰˜ä¹¦.*?ä»£ç†\s+(.*?)å•†æ ‡\s*çš„\s*å¦‚ä¸‹.*?äº‹å®œ', page_text, re.DOTALL)
                # å¤‡é€‰æ›´å®½æ¾çš„æ­£åˆ™ (å¦‚æœä¸Šé¢çš„ä¸å¤Ÿç²¾ç¡®):
                # tm_name_match = re.search(r'å•†æ ‡ä»£ç†å§”æ‰˜ä¹¦.*?ä»£ç†\s*(.*?)(?:å•†æ ‡\s*çš„\s*å¦‚ä¸‹|å•†æ ‡çš„)', page_text, re.DOTALL)

                tm_name = tm_name_match.group(1).strip() if tm_name_match else ""

                if not tm_name:
                    # å¦‚æœä¸Šé¢çš„æ­£åˆ™æ²¡æ‰¾åˆ°ï¼Œå°è¯•ä¸€ä¸ªæ›´å®½æ¾çš„å¤‡é€‰æ–¹æ¡ˆ
                    # æŸ¥æ‰¾ "ä»£ç†" å’Œ "å•†æ ‡" ä¹‹é—´çš„ä»»ä½•å†…å®¹
                    fallback_match = re.search(r'ä»£ç†\s+(.*?)\s*å•†æ ‡', page_text)
                    if fallback_match:
                        tm_name = fallback_match.group(1).strip()

                if not tm_name:
                    st.warning(f"è­¦å‘Šï¼šåœ¨ä¸Šä¼ çš„æ–‡ä»¶ '{uploaded_file.name}' çš„ç¬¬ {page_num + 1} é¡µå§”æ‰˜ä¹¦ä¸­æœªæ‰¾åˆ°å•†æ ‡åç§°ã€‚")
                    # å³ä½¿æ²¡æ‰¾åˆ°åç§°ï¼Œä¹Ÿæ¸…ç©ºpending_categoriesï¼Œé¿å…ç´¯ç§¯é”™è¯¯
                    pending_categories.clear()
                    continue
                # --- ä¿®æ”¹ç‚¹ç»“æŸ ---

                # æå–å§”æ‰˜ä¹¦æ—¥æœŸ
                date_match = re.search(r"(\d{4}å¹´\s*\d{1,2}æœˆ\s*\d{1,2}æ—¥)", page_text)
                if date_match:
                    final_date = date_match.group(1).replace(" ", "") # æ›´æ–°ä¸ºæœ€æ–°çš„å§”æ‰˜ä¹¦æ—¥æœŸ

                # å…³è”ï¼šå°†æš‚å­˜çš„ç±»åˆ«ä¸å½“å‰å•†æ ‡åç»„åˆ
                if pending_categories:
                    for category in pending_categories:
                        trademarks_with_categories.append({
                            "å•†æ ‡åç§°": tm_name,
                            "ç±»åˆ«": category
                        })
                    # print(f"  -> å…³è”å•†æ ‡ '{tm_name}' ä¸ç±»åˆ« {pending_categories}") # è°ƒè¯•ä¿¡æ¯
                    pending_categories.clear() # å…³è”åæ¸…ç©ºæš‚å­˜åŒº
                else:
                    # ç‰¹æ®Šæƒ…å†µï¼šå§”æ‰˜ä¹¦é¡µé¢æ²¡æœ‰å‰é¢çš„ç±»åˆ«ä¿¡æ¯ï¼Œéœ€è¦æ‰‹åŠ¨è¾“å…¥
                    # æˆ‘ä»¬åœ¨è¿™é‡Œæ ‡è®°è¿™ä¸ªå•†æ ‡éœ€è¦æ‰‹åŠ¨è¾“å…¥ç±»åˆ«
                    # çœŸæ­£çš„æ‰‹åŠ¨è¾“å…¥é€»è¾‘åœ¨ batch_process_pdfs ä¸­å¤„ç†ã€‚
                    # ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬å¯ä»¥å…ˆæ·»åŠ ä¸€ä¸ªå ä½ç¬¦ï¼Œæˆ–è€…åœ¨ä¸»é€»è¾‘ä¸­ç‰¹æ®Šå¤„ç†ã€‚
                    # è¿™é‡Œæˆ‘ä»¬é€‰æ‹©æ·»åŠ ä¸€ä¸ªç‰¹æ®Šæ ‡è®°
                    trademarks_with_categories.append({
                        "å•†æ ‡åç§°": tm_name,
                        "ç±»åˆ«": "MANUAL_INPUT_REQUIRED" # ç‰¹æ®Šæ ‡è®°
                    })
                    st.info(f"æç¤ºï¼šä¸Šä¼ çš„æ–‡ä»¶ '{uploaded_file.name}' ä¸­çš„å•†æ ‡ '{tm_name}' æœªæ‰¾åˆ°è‡ªåŠ¨å…³è”çš„ç±»åˆ«ï¼Œéœ€è¦æ‰‹åŠ¨è¾“å…¥ã€‚")

        # --- å¤„ç†ç»“æŸåï¼Œæ£€æŸ¥æ˜¯å¦è¿˜æœ‰æœªå…³è”çš„ç±»åˆ« ---
        if pending_categories:
            st.warning(f"è­¦å‘Šï¼šä¸Šä¼ çš„æ–‡ä»¶ '{uploaded_file.name}' å¤„ç†å®Œæ¯•ï¼Œä½†ä»æœ‰æœªå…³è”çš„ç±»åˆ« {pending_categories}ã€‚è¿™äº›ç±»åˆ«å°†è¢«å¿½ç•¥ã€‚")

    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        os.unlink(tmp_file_path)

    return {
        "ç”³è¯·äºº": applicant,
        "ç»Ÿä¸€ç¤¾ä¼šä¿¡ç”¨ä»£ç ": unified_credit_code,
        "æ—¥æœŸ": final_date,
        "å•†æ ‡åˆ—è¡¨": trademarks_with_categories,
        "äº‹å®œç±»å‹": "å•†æ ‡æ³¨å†Œç”³è¯·"
    }


def process_uploaded_files(uploaded_files, template_word_path, template_excel_path, output_dir, default_agent_fee):
    """
    å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶åˆ—è¡¨ã€‚
    :param uploaded_files: Streamlit UploadedFile å¯¹è±¡åˆ—è¡¨
    :param template_word_path: Wordæ¨¡æ¿æ–‡ä»¶è·¯å¾„
    :param template_excel_path: Excelæ¨¡æ¿æ–‡ä»¶è·¯å¾„
    :param output_dir: è¾“å‡ºæ–‡ä»¶ç›®å½•
    :param default_agent_fee: é»˜è®¤ä»£ç†è´¹
    :return: å¤„ç†ç»“æœå­—å…¸ {'success': bool, 'word_count': int, 'errors': list, ...}
    """
    results = {
        'success': False,
        'word_count': 0,
        'errors': [],
    }

    if not os.path.exists(template_word_path):
        results['errors'].append(f"æ‰¾ä¸åˆ°Wordæ¨¡æ¿æ–‡ä»¶: {template_word_path}")
        return results
    if not os.path.exists(template_excel_path):
        results['errors'].append(f"æ‰¾ä¸åˆ°Excelæ¨¡æ¿æ–‡ä»¶: {template_excel_path}")
        return results

    # --- æŒ‰ç”³è¯·äººåˆ†ç»„æ•°æ® ---
    applicant_data_groups = defaultdict(list)
    success_count = 0

    # ç¬¬ä¸€æ­¥ï¼šéå†æ‰€æœ‰ä¸Šä¼ çš„PDFï¼Œæå–æ•°æ®å¹¶æŒ‰ç”³è¯·äººåˆ†ç»„
    for uploaded_file in uploaded_files:
        try:
            # print(f"æ­£åœ¨æå–æ•°æ®ï¼š{uploaded_file.name}") # å¯ç”¨äºè°ƒè¯•
            data = extract_pdf_data_streamlit(uploaded_file)
            applicant = data["ç”³è¯·äºº"]
            applicant_data_groups[applicant].append(data)
            success_count += 1
        except Exception as e:
            error_msg = f"æå–æ•°æ®å¤±è´¥ '{uploaded_file.name}': {str(e)}"
            results['errors'].append(error_msg)
            st.error(error_msg) # åœ¨UIä¸Šæ˜¾ç¤ºé”™è¯¯

    # print(f"\næ•°æ®æå–å®Œæˆï¼Œå…±å¤„ç† {success_count} ä¸ªæ–‡ä»¶ã€‚") # å¯ç”¨äºè°ƒè¯•

    # ç¬¬äºŒæ­¥ï¼šä¸ºæ¯ä¸ªç”³è¯·äººç»„ç”Ÿæˆä¸€ä¸ªè¯·æ¬¾å•
    generated_word_count = 0
    all_applicants_summary = []
    # generated_files = [] # è®°å½•ç”Ÿæˆçš„æ–‡ä»¶

    for applicant, data_list in applicant_data_groups.items():
        try:
            # print(f"\næ­£åœ¨ä¸ºç”³è¯·äºº '{applicant}' ç”Ÿæˆè¯·æ¬¾å•...") # å¯ç”¨äºè°ƒè¯•
            merged_trademarks = []
            latest_date = "N/A"
            unified_credit_code = "N/A"

            # --- ä¿®æ”¹ç‚¹ï¼šå¤„ç†éœ€è¦æ‰‹åŠ¨è¾“å…¥ç±»åˆ«çš„å•†æ ‡ ---
            manual_input_needed = False
            manual_input_data = [] # å­˜å‚¨éœ€è¦æ‰‹åŠ¨è¾“å…¥çš„å•†æ ‡

            for data in data_list:
                for tm_item in data["å•†æ ‡åˆ—è¡¨"]:
                    if tm_item["ç±»åˆ«"] == "MANUAL_INPUT_REQUIRED":
                        manual_input_needed = True
                        manual_input_data.append(tm_item["å•†æ ‡åç§°"])
                        # ä¸ºäº†åç»­å¤„ç†ï¼Œæˆ‘ä»¬æš‚æ—¶ä¸æ·»åŠ åˆ° merged_trademarks
                    else:
                        merged_trademarks.append(tm_item)

                # æ›´æ–°æœ€æ–°æ—¥æœŸå’Œç»Ÿä¸€ç¤¾ä¼šä¿¡ç”¨ä»£ç 
                if data["æ—¥æœŸ"] != "N/A":
                     latest_date = data["æ—¥æœŸ"]
                if data["ç»Ÿä¸€ç¤¾ä¼šä¿¡ç”¨ä»£ç "] != "N/A":
                     unified_credit_code = data["ç»Ÿä¸€ç¤¾ä¼šä¿¡ç”¨ä»£ç "]

            # --- å¦‚æœéœ€è¦æ‰‹åŠ¨è¾“å…¥ç±»åˆ« ---
            if manual_input_needed:
                st.warning(f"ç”³è¯·äºº '{applicant}' æœ‰å•†æ ‡éœ€è¦æ‰‹åŠ¨è¾“å…¥ç±»åˆ«: {', '.join(manual_input_data)}ã€‚è¿™äº›å•†æ ‡å·²è¢«è·³è¿‡å¤„ç†ã€‚")

            # --- å‡†å¤‡åˆå¹¶åçš„æ•°æ®ç»“æ„ ---
            merged_data = {
                "ç”³è¯·äºº": applicant,
                "ç»Ÿä¸€ç¤¾ä¼šä¿¡ç”¨ä»£ç ": unified_credit_code,
                "æ—¥æœŸ": latest_date, # ä½¿ç”¨æœ€æ–°çš„æ—¥æœŸ
                "å•†æ ‡åˆ—è¡¨": merged_trademarks, # åŒ…å«è‡ªåŠ¨å’Œæ‰‹åŠ¨è¾“å…¥çš„å•†æ ‡ (æ‰‹åŠ¨è¾“å…¥çš„å·²è¢«è·³è¿‡)
                "äº‹å®œç±»å‹": "å•†æ ‡æ³¨å†Œç”³è¯·"
            }

            # å¦‚æœæ²¡æœ‰æœ‰æ•ˆçš„å•†æ ‡é¡¹ç›®ï¼Œè·³è¿‡ç”Ÿæˆè¯·æ¬¾å•
            categories = merged_trademarks
            num_items = len(categories)
            if num_items == 0:
                warning_msg = f"è­¦å‘Šï¼šç”³è¯·äºº '{applicant}' æ²¡æœ‰æœ‰æ•ˆçš„å•†æ ‡é¡¹ç›®ï¼Œè·³è¿‡ç”Ÿæˆè¯·æ¬¾å•ã€‚"
                results['errors'].append(warning_msg)
                st.warning(warning_msg)
                continue

            # è·å–ä»£ç†è´¹ (å¯¹åŒä¸€ç”³è¯·äººåªéœ€è·å–ä¸€æ¬¡)
            # ç®€åŒ–å¤„ç†ï¼šä½¿ç”¨ä¼ å…¥çš„é»˜è®¤å€¼ã€‚
            agent_fee = default_agent_fee

            # ç”ŸæˆWord
            word_file_path = create_word_doc(merged_data, agent_fee, categories, template_word_path, output_dir)
            # generated_files.append(word_file_path) # è®°å½•ç”Ÿæˆçš„æ–‡ä»¶
            # print(f"å·²ç”Ÿæˆè¯·æ¬¾å•: {os.path.basename(word_file_path)}") # å¯ç”¨äºè°ƒè¯•
            generated_word_count += 1

            # æ”¶é›†æ•°æ®åˆ°æ±‡æ€»åˆ—è¡¨ (ä¸ºExcel)
            total_official = num_items * 270
            total_agent = num_items * agent_fee
            total_subtotal = total_official + total_agent
            all_applicants_summary.append({
                "ç”³è¯·äºº": applicant,
                "ç»Ÿä¸€ç¤¾ä¼šä¿¡ç”¨ä»£ç ": unified_credit_code,
                "æ—¥æœŸ": latest_date,
                "æ€»å®˜è´¹": total_official,
                "æ€»ä»£ç†è´¹": total_agent,
                "æ€»è®¡": total_subtotal
            })

        except Exception as e:
             error_msg = f"ä¸ºç”³è¯·äºº '{applicant}' ç”Ÿæˆè¯·æ¬¾å•æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
             results['errors'].append(error_msg)
             st.error(error_msg)


    # ç¬¬ä¸‰æ­¥ï¼šç”ŸæˆExcelæ±‡æ€»æ–‡ä»¶
    if all_applicants_summary:
        try:
            wb = load_workbook(template_excel_path)
            ws = wb.active
            row_num = 2
            for applicant_data in all_applicants_summary:
                # å®˜è´¹è¡Œ
                ws[f'B{row_num}'] = applicant_data["ç”³è¯·äºº"]
                ws[f'C{row_num}'] = applicant_data["ç»Ÿä¸€ç¤¾ä¼šä¿¡ç”¨ä»£ç "]
                ws[f'G{row_num}'] = applicant_data["æ€»å®˜è´¹"]
                ws[f'H{row_num}'] = applicant_data["æ€»å®˜è´¹"]
                ws[f'I{row_num}'] = applicant_data["æ€»è®¡"]
                ws[f'Q{row_num}'] = applicant_data["æ—¥æœŸ"]
                row_num += 1
                # ä»£ç†è´¹è¡Œ
                ws[f'B{row_num}'] = applicant_data["ç”³è¯·äºº"]
                ws[f'C{row_num}'] = applicant_data["ç»Ÿä¸€ç¤¾ä¼šä¿¡ç”¨ä»£ç "]
                ws[f'G{row_num}'] = applicant_data["æ€»ä»£ç†è´¹"]
                ws[f'H{row_num}'] = applicant_data["æ€»ä»£ç†è´¹"]
                ws[f'I{row_num}'] = applicant_data["æ€»è®¡"]
                ws[f'Q{row_num}'] = applicant_data["æ—¥æœŸ"]
                row_num += 1
            # ä½¿ç”¨ç¬¬ä¸€ä¸ªç”³è¯·äººçš„æ—¥æœŸæˆ–å½“å‰æ—¥æœŸä½œä¸ºæ–‡ä»¶åæ—¥æœŸ
            summary_date = all_applicants_summary[0]["æ—¥æœŸ"] if all_applicants_summary else "N/A"
            excel_filename = f"å‘ç¥¨ç”³è¯·è¡¨-{summary_date}.xlsx"
            excel_file_path = os.path.join(output_dir, excel_filename)
            wb.save(excel_file_path)
            # generated_files.append(excel_file_path) # è®°å½•ç”Ÿæˆçš„æ–‡ä»¶
            # print(f"\nå·²ç”ŸæˆExcelæ±‡æ€»æ–‡ä»¶: {excel_filename}") # å¯ç”¨äºè°ƒè¯•
        except Exception as e:
             error_msg = f"ç”ŸæˆExcelæ±‡æ€»æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
             results['errors'].append(error_msg)
             st.error(error_msg)
    else:
        warning_msg = "\næ²¡æœ‰æœ‰æ•ˆçš„ç”³è¯·äººæ•°æ®ç”¨äºç”ŸæˆExcelæ±‡æ€»æ–‡ä»¶ã€‚"
        results['errors'].append(warning_msg)
        st.warning(warning_msg)

    # print(f"\nå¤„ç†å®Œæˆï¼å…±å°è¯•ç”Ÿæˆ {len(applicant_data_groups)} ä¸ªç”³è¯·äººçš„è¯·æ¬¾å•ï¼ŒæˆåŠŸç”Ÿæˆ {generated_word_count} ä¸ªWordæ–‡ä»¶å’Œ 1 ä¸ªExcelæ±‡æ€»æ–‡ä»¶") # å¯ç”¨äºè°ƒè¯•
    results['success'] = True # å¦‚æœèµ°åˆ°è¿™ä¸€æ­¥ï¼Œå°±ç®—æœ‰è­¦å‘Šä¹Ÿè®¤ä¸ºåŸºæœ¬æˆåŠŸ
    results['word_count'] = generated_word_count
    # results['generated_files'] = generated_files
    return results

# --- Streamlit åº”ç”¨ä¸»é€»è¾‘ ---

# 1. æ–‡ä»¶ä¸Šä¼ 
uploaded_files = st.file_uploader("ğŸ“ è¯·é€‰æ‹©PDFæ–‡ä»¶", type=['pdf'], accept_multiple_files=True)

# 2. ä»£ç†è´¹è¾“å…¥
default_agent_fee = st.number_input("ğŸ’° è¯·è¾“å…¥é»˜è®¤ä»£ç†è´¹ï¼ˆå…ƒ/é¡¹ï¼‰", min_value=0, value=1000, step=100)

# 3. å¤„ç†æŒ‰é’®
if st.button("ğŸš€ å¼€å§‹å¤„ç†"):
    if not uploaded_files:
        st.warning("âš ï¸ è¯·è‡³å°‘ä¸Šä¼ ä¸€ä¸ªPDFæ–‡ä»¶ã€‚")
    else:
        # --- å¤„ç†é€»è¾‘ ---
        with st.spinner("â³ æ­£åœ¨å¤„ç†æ–‡ä»¶..."):
            try:
                # åˆ›å»ºä¸´æ—¶è¾“å‡ºç›®å½•
                OUTPUT_DIR = tempfile.mkdtemp()

                # è°ƒç”¨æ ¸å¿ƒå¤„ç†å‡½æ•°
                processing_results = process_uploaded_files(
                    uploaded_files=uploaded_files,
                    template_word_path="è¯·æ¬¾å•.docx", # å‡è®¾æ¨¡æ¿åœ¨æ ¹ç›®å½•
                    template_excel_path="å‘ç¥¨ç”³è¯·è¡¨.xlsx", # å‡è®¾æ¨¡æ¿åœ¨æ ¹ç›®å½•
                    output_dir=OUTPUT_DIR,
                    default_agent_fee=default_agent_fee
                )

                # --- æ˜¾ç¤ºå¤„ç†ç»“æœ ---
                if processing_results["success"]:
                    st.success(f"âœ… å¤„ç†å®Œæˆï¼æˆåŠŸç”Ÿæˆäº† {processing_results['word_count']} ä¸ªè¯·æ¬¾å•å’Œ 1 ä¸ªå‘ç¥¨ç”³è¯·è¡¨ã€‚")

                    # --- æä¾›æ–‡ä»¶ä¸‹è½½ ---
                    # 1. æ‰“åŒ…æ‰€æœ‰ç”Ÿæˆçš„æ–‡ä»¶ä¸ºä¸€ä¸ª ZIP
                    zip_buffer = io.BytesIO()
                    with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
                        for root, dirs, files in os.walk(OUTPUT_DIR):
                            for file in files:
                                file_path = os.path.join(root, file)
                                # å°†æ–‡ä»¶æ·»åŠ åˆ° ZIPï¼Œä¿æŒç›®å½•ç»“æ„ï¼ˆå¦‚æœéœ€è¦ï¼‰æˆ–æ‰å¹³åŒ–
                                arcname = os.path.relpath(file_path, OUTPUT_DIR)
                                zip_file.write(file_path, arcname)

                    zip_buffer.seek(0) # é‡ç½®ç¼“å†²åŒºæŒ‡é’ˆ

                    st.download_button(
                        label="ğŸ“¥ ä¸‹è½½æ‰€æœ‰ç”Ÿæˆçš„æ–‡ä»¶ (ZIP)",
                        data=zip_buffer,
                        file_name="generated_files.zip",
                        mime="application/zip"
                    )

                else:
                    st.error("âŒ å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ã€‚")
                    if processing_results.get("errors"):
                        st.text_area("ğŸ” é”™è¯¯è¯¦æƒ…:", value="\n".join(processing_results["errors"]), height=200)

            except Exception as e:
                st.error(f"ğŸ’¥ åº”ç”¨è¿è¡Œæ—¶å‘ç”Ÿæœªé¢„æœŸçš„é”™è¯¯: {e}")
            finally:
                # æ¸…ç†ä¸´æ—¶è¾“å‡ºç›®å½•
                if 'OUTPUT_DIR' in locals() and os.path.exists(OUTPUT_DIR):
                    shutil.rmtree(OUTPUT_DIR)

# --- README æˆ–è¯´æ˜ä¿¡æ¯ ---
st.markdown("---")
st.markdown("""
### ğŸ“ ä½¿ç”¨è¯´æ˜
1.  å°† `è¯·æ¬¾å•.docx` å’Œ `å‘ç¥¨ç”³è¯·è¡¨.xlsx` æ¨¡æ¿æ–‡ä»¶ä¸ `app.py` æ”¾åœ¨åŒä¸€ç›®å½•ä¸‹ã€‚
2.  åœ¨å·¦ä¾§ä¸Šä¼ ä¸€ä¸ªæˆ–å¤šä¸ªPDFå•†æ ‡ç”³è¯·æ–‡ä»¶ã€‚
3.  è¾“å…¥é»˜è®¤ä»£ç†è´¹ã€‚
4.  ç‚¹å‡»â€œğŸš€ å¼€å§‹å¤„ç†â€ã€‚
5.  å¤„ç†å®Œæˆåï¼Œç‚¹å‡»â€œğŸ“¥ ä¸‹è½½æ‰€æœ‰ç”Ÿæˆçš„æ–‡ä»¶ (ZIP)â€è·å–ç»“æœã€‚
""")
